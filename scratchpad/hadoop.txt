=====================================================================================================================================================
#	Definitions
=====================================================================================================================================================
#.	Big Data : Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate. 
#.	Hadoop : Solution for deciphering the avalanche of Big Data

=====================================================================================================================================================
#	Hadoop admin
=====================================================================================================================================================
#.	Single node setup : https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html
#.	Set following in bash profile,
	-------------------------------------------------------------------------------------------------------------------------------------------------
	# Hadoop variables
	export HADOOP_HOME=/root/hadoop
	export HADOOP_INSTALL=/root/hadoop
	export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
	export HADOOP_COMMON_HOME=$HADOOP_INSTALL
	export HADOOP_HDFS_HOME=$HADOOP_INSTALL
	export YARN_HOME=$HADOOP_INSTALL
	#Also used by Apache Spark
	export HADOOP_CONF_DIR=$HADOOP_INSTALL/etc/hadoop
	
	# Packer.io/Maven/Ant/Splunk executables in PATH
	export PATH=$HOME/bin:/root/downloads/packer:/root/maven/bin:/root/amit/apache-ant-1.9.4/bin:/opt/chef/embedded/bin:$SPLUNK_HOME/bin:/usr/java/jdk1.7.0_17/bin/:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbin:$PATH
	-------------------------------------------------------------------------------------------------------------------------------------------------
#.	Start NameNode and Secondry NameNode using command:  sbin/start-dfs.sh and access namenode at http://192.168.148.76:50070/
#.	Start Yarn using command : sbin/start-yarn.sh Resource Manager/Yarn : http://192.168.148.76:8088/cluster
#.	JAVA issue 
	#.	cd /usr/lib/jvm
	#.	sudo ln -s /usr/java/jdk1.7.0_17/ java
#.	Hadoop connection refused issue : http://stackoverflow.com/questions/29192088/java-net-connectexception-connection-refused-in-hadoop-while-using-shell-comman

=====================================================================================================================================================
#	HDFS Commands
=====================================================================================================================================================
#.	Create new directory on hdfs : hdfs dfs -mkdir /user
#.	List the contents of a directory : hdfs dfs -ls /user
#.	File copy :  hadoop fs -copyFromLocal /root/amit/hadoop-book/input/docs/quangle.txt hdfs://localhost:9000/user/amit/quangle.txt
#.	File put : hadoop fs -put input/ncdc/sample.txt /user/test
#.	File print : hadoop fs -cat /user/root/output/part-r-00000
=====================================================================================================================================================
#	YARN
=====================================================================================================================================================
#.	To check status of an application : yarn logs -applicationId application_1453903755565_0008 

=====================================================================================================================================================
#	My development enviornment details
=====================================================================================================================================================
#.	Run job on local 
	#.	export HADOOP_CLASSPATH=ds.jar 
	#.	hadoop MaxTemperature ../../input/ncdc/sample.txt output
	#.	hadoop com.inbravo.hadoop.MaxTemperature ../hadoop-book/input/ncdc/sample.txt output
	#.	Output is available at /output/part-r-00000
#.	Run job on local cluster
	#.	Put sample file on hdfs : hadoop fs -put input/ncdc/sample.txt /user/test
	#.	Now run job with cluster : hadoop com.inbravo.hadoop.MaxTemperatureDriver -conf conf/hadoop-cluster.xml /user/test/sample.txt output
	#.	Output is available at : hadoop fs -cat /user/root/output/part-r-00000

=====================================================================================================================================================
#	Linux profile details to support hadoop/hive/spark
=====================================================================================================================================================
# Set Java enviornment
# For JDK 8 use path: /home/impadmin/opensource/java/jdk1.8.0_45
# For JDK 7 use path: /home/impadmin/opensource/java/jdk1.7.0_76
export JAVA_HOME=/home/impadmin/opensource/java/jdk1.8.0_45
export ANT_HOME=/home/impadmin/opensource/ant
export SBT_HOME=/home/impadmin/opensource/sbt
export MAVEN_HOME=/home/impadmin/opensource/maven-3.3.9
export TOMCAT_HOME=/home/impadmin/opensource/tomcat-8

# HDFS settings
export HADOOP_HOME=/home/impadmin/opensource/hadoop-2.6.0
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HIVE_HOME=/home/impadmin/opensource/hive-1.2.1
export SQOOP_HOME=/home/impadmin/opensource/sqoop-1.4.6
export SPARK_HOME=/home/impadmin/opensource/spark-1.6


# Update PATH variable to include all libraries
export PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin:$HADOOP_HOME/bin:$SQOOP_HOME/bin:$TOMCAT_HOME/bin:$HIVE_HOME/bin:$SPARK_HOME /bin:$ANT_HOME/bin:$SBT_HOME/bin

